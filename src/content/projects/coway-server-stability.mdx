---
title: 'IoCare+ 배포/서버 안정화 및 메모리 관리'
order: 9
startDate: '2023-12-01'
endDate: '2024-08-31'
description: 'Next.js 메모리 누수 해결 및 PM2 클러스터 모드 기반 무중단 재시작 시스템 구축'
tags: ['Next.js', 'PM2', 'AWS EC2', 'AWS Auto Scaling', 'AWS CodeDeploy', 'Nginx']
company: "코웨이"
---

## 프로젝트 개요

IoCare+ 서비스의 프로덕션 환경에서 발생한 Next.js 메모리 누수 문제를 해결하고, PM2 클러스터 모드 기반의 안정적인 서버 운영 시스템을 구축했습니다.

## 서버 인프라 구성

IoCare+ 서비스는 AWS 클라우드 환경에서 고가용성과 무중단 운영을 위한 인프라 구조로 구성되어 있습니다.

### 인프라 아키텍처

```
[AWS CloudFront (CDN)]
   ↓
[AWS ALB (Application Load Balancer)]
   ↓
[AWS Auto Scaling Group]
   ↓
[AWS EC2 Instance (Ubuntu)]
   ↓
[Nginx (Reverse Proxy & Load Balancer)]
   ↓
[PM2 Cluster Mode (4~8 Workers)]
   ↓
[Next.js App Router (SSR/CSR)]
```

### EC2 인스턴스 구성

- **OS**: Ubuntu
- **Auto Scaling**: 최소 2대 ~ 최대 4대
- **배포 방식**: AWS CodeDeploy를 통한 Blue/Green 배포

### PM2 클러스터 모드

- **로드 밸런싱**: Round-robin 방식
- **무중단 재시작**: Graceful Reload
- **프로세스 모니터링**: 메모리 사용량 기반 자동 재시작

## 문제 상황

### 메모리 누수 현상

프로덕션 환경에서 Next.js 서버의 메모리 사용량이 시간이 지남에 따라 지속적으로 증가하는 문제가 발생했습니다.

#### 증상
- 서버 시작 후 메모리 사용량: **약 500MB**
- 일정 시간 운영 후: **2GB 초과** → 서버 응답 지연 및 불안정

#### 원인 분석
- Next.js App Router의 서버 컴포넌트 캐싱
- SWR 캐시 누적
- Jotai 전역 상태 메모리 누적
- i18n 서버 리소스 관리
- Instrumentation

## 해결 방법

### PM2 클러스터 모드 + 메모리 기반 재시작

PM2의 클러스터 모드와 메모리 모니터링 기능을 활용하여 무중단 재시작 시스템을 구축했습니다.

#### PM2 설정 (ecosystem.config.js)

```javascript
module.exports = {
  apps: [
    {
      name: 'iocare-plus',
      script: 'node_modules/next/dist/bin/next',
      args: 'start',
      instances: 'max', // CPU 코어 수만큼 프로세스 생성
      exec_mode: 'cluster',
      max_memory_restart: '2G', // 2GB 초과 시 자동 재시작
      autorestart: true,
      watch: false,
      env: {
        NODE_ENV: 'production',
        PORT: 3000,
      },
    },
  ],
};
```

### 무중단 재시작 프로세스

PM2 클러스터 모드의 **무중단 재시작(Zero-Downtime Reload)** 메커니즘:

1. **메모리 임계치 도달**: 특정 워커 프로세스가 2GB 초과
2. **새 워커 시작**: PM2가 새로운 워커 프로세스 생성
3. **트래픽 전환**: Nginx가 새 워커로 트래픽 라우팅
4. **기존 워커 종료**: 기존 워커의 활성 요청 완료 후 graceful shutdown
5. **서비스 연속성**: 사용자는 중단 없이 서비스 이용

### Nginx 로드 밸런싱 설정

```nginx
upstream iocare_plus {
    least_conn; # 최소 연결 수 기반 로드 밸런싱
    server 127.0.0.1:3000;
    server 127.0.0.1:3001;
    keepalive 64;
}
```

## 모니터링 및 운영

### PM2 모니터링 명령어

```bash
# 프로세스 상태 확인
pm2 status

# 메모리 사용량 실시간 모니터링
pm2 monit

# 로그 확인
pm2 logs iocare-plus

# 재시작 이력 확인
pm2 describe iocare-plus
```

## 성과

### 서버 안정성 개선

| 항목 | 개선 전 | 개선 후 |
|------|---------|---------|
| **평균 가동 시간** | 24시간 (메모리 부족으로 수동 재시작) | 무제한 (자동 재시작) |
| **서비스 중단 시간** | 재시작 시 30초~1분 | 0초 (무중단 재시작) |
| **메모리 관리** | 수동 모니터링 및 재시작 | 자동 임계치 기반 재시작 |
| **장애 복구 시간** | 평균 5분 (수동 대응) | 즉시 (자동 재시작) |

### 운영 효율성

- **자동 복구**: 프로세스 크래시 시 자동 재시작
- **로드 밸런싱**: 클러스터 모드로 CPU 자원 효율적 활용
- **운영 부담 감소**: 수동 모니터링 및 재시작 작업 제거

## 기술적 의사결정

### PM2 클러스터 모드 선택 이유

다른 대안 대비 PM2 클러스터 모드를 선택한 이유:

| 방식 | 장점 | 선택 이유 |
|------|------|-----------|
| **PM2 클러스터** | 기존 EC2 환경 유지, 빠른 적용 | **즉시 적용 가능, 비용 효율적** |

### 2GB 임계치 설정 근거

**백엔드 서버 인프라 자문 및 협업**

백엔드 인프라 팀과 협업하여 AWS EC2 인스턴스 유형 및 서비스 운용 적정 메모리 임계치를 산정했습니다.

## 트러블 슈팅

### AWS CodeDeploy 배포 시 디스크 용량 부족 문제

#### 문제 상황

CodeDeploy를 통한 배포 과정에서 EC2 인스턴스의 디스크 용량이 지속적으로 증가하여 배포 실패가 발생했습니다.

**증상**:
- 배포 반복 시 디스크 사용량 증가
- `/opt/codedeploy-agent/deployment-root` 디렉토리 용량 증가
- 디스크 full로 인한 배포 실패

#### 원인 분석

##### 1. EBS 볼륨 용량 부족

AWS EC2 인스턴스의 기본 EBS(Elastic Block Store) 볼륨 설정:

- **기본 용량**: 8GB (인스턴스 생성 시 기본값)
- **프리 티어**: 30GB까지 무료 제공
- **문제**: 기본 8GB로는 OS, 애플리케이션, 배포 이력 저장 공간 부족

##### 2. CodeDeploy 배포 이력 누적

CodeDeploy 에이전트는 배포 이력(revision)을 EC2 인스턴스에 저장합니다.

- **기본 설정**: `max_revisions: 5` (최근 5개 배포 파일 보관)
- **배포 파일 저장 위치**: `/opt/codedeploy-agent/deployment-root/`
- **문제**: 배포물 크기가 크고 배포 빈도가 높을 경우 디스크 공간 부족

#### 해결 방법

##### 1. EBS 볼륨 용량 증설 (8GB → 30GB)

기존 인스턴스의 EBS 볼륨 용량을 8GB에서 30GB로 확대

##### 2. 시작 템플릿(Launch Template) 생성 및 관리

Auto Scaling Group에서 새로 생성되는 인스턴스가 30GB EBS를 사용하도록 시작 템플릿을 구성했습니다.

1. 시작 템플릿 생성
2. Auto Scaling Group에 시작 템플릿 연결
3. 시작 템플릿 버전 관리

##### 3. CodeDeploy 배포 이력 관리

CodeDeploy 에이전트 설정 파일에서 `max_revisions` 값을 조정하여 보관 이력 수를 제한했습니다.

###### 3-1. 설정 파일 수정

```bash
# CodeDeploy 에이전트 설정 파일 열기
sudo vi /etc/codedeploy-agent/conf/codedeployagent.yml
```

###### 3-2. max_revisions 값 변경

```yaml
---
:log_aws_wire: false
:log_dir: '/var/log/aws/codedeploy-agent/'
:pid_dir: '/opt/codedeploy-agent/state/.pid/'
:program_name: codedeploy-agent
:root_dir: '/opt/codedeploy-agent/deployment-root'
:verbose: false
:wait_between_runs: 1
:max_revisions: 2  # 기본값 5 → 2로 변경
```

## 관련 프로젝트

IoCare+ 서비스의 전체 기술 스택 및 구현 상세 내용은 [IoCare+ IoT 글로벌 서비스](/projects/coway-iocare-plus-service)를 참고하세요.
